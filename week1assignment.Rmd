```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(tidyverse)
library(faraway)
library(simex)
library(magrittr)
library(moments)
library(GGally)
library(knitr)
library(kableExtra)
library(caret)
library(doSNOW)
money.ball <- read_csv('C:\\Users\\Brian\\Desktop\\GradClasses\\Summer18\\621\\621week1\\moneyball-training-data.csv')
```

<div class='jumbotron'>
  <h2 class='display-3 text-uppercase'>Week 1 Assignment</h2>
  <h4 class='right text-uppercase'>By Brian Weinfeld</h4>
  <div class='clearfix'></div>
  <h5 class='right text-uppercase'>June 7th, 2018</h5>
</div>

<div class='page-header text-uppercase'>
  <h3>DATA EXPLORATION</h3>
</div>

<div class='well'>
The money ball data set contains 2276 observations representing a baseball team's season with 17 integer variables representing various offensive and defensive baseball statistics collected over the past 100+ years. The below box plots demonstrates the wide range of values that the data holds. Most of the variables contain significant outliers. Note the log scale.
</div>

```{r, warning=FALSE}
money.ball %>%
  select(-INDEX) %>%
  gather() %>%
  ggplot() +
    geom_boxplot(aes(key, value)) +
    scale_y_log10() +
    coord_flip()
```

<div class='well'>
There is a sizeable amount of missing data as well. For example, the 'hit by pitch' category is missing over 90% of its data. There was a period of time in baseball where hitting a batter with a pitch was considered a ball (and not a free base). This may be part of the reason that this statistic was not tracked.
</div>

```{r}
money.ball %>%
  map_dbl(~sum(is.na(.))/nrow(money.ball)) %>%
  kable()
```

<div class='page-header text-uppercase'>
  <h3>DATA PREPARATION</h3>
</div>

<div class='well'>
The TEAM_BASERUN_CS statistic and TEAM_BATTING_HBP statistics are missing a sizeable amount of observations. When a statistic is missing a few observation (for example, TEAM_PITCHING_SO) modifications can be made to address this issue. However, as such a large proportion are missing I think its better to ignore these columns.
</div>

```{r}
money.ball %<>%
  select(-TEAM_BASERUN_CS, -TEAM_BATTING_HBP)
```

<div class='well'>
There are 19 observations that are missing large chunks of information. This would require estimating so much of their information that I do not believe it is representative of the original team. These observations will be removed.
</div>

```{r}
money.ball %>%
  filter(TEAM_BATTING_SO == 0) %>%
  select(INDEX, TEAM_BATTING_SO, TEAM_BASERUN_SB, TEAM_PITCHING_SO, TEAM_FIELDING_DP) %>%
  kable('html') %>%
  kable_styling(bootstrap_options = c('striped', 'hover'))

money.ball %<>%
  filter(TEAM_BATTING_SO != 0)
```

<div class='well'>
There are 4 observations whose values lead me to believe they are mistakes. This includes a team with 0 wins and 3 teams with certain statistics much larger than any other team. I believe these may be extrapolation errors. These observations have been removed.
</div>

```{r}
money.ball %<>%
  filter(!INDEX %in% c(1347, 2380, 1494, 1769))
```

<div class='well'>
Next, we need to address the missing values from the above noted columns. We will use the caret package to make predicted values for the missing pieces of information. Finally, we remove the index column as it is no longer needed.
</div>

```{r}
dummy.vars <- dummyVars(~ ., data = money.ball[, c(-1, -2)])
train.dummy <- predict(dummy.vars, money.ball[, c(-1, -2)])


pre.process <- preProcess(train.dummy, method='bagImpute')
imputed.data <- predict(pre.process, train.dummy)

money.ball %<>%
  mutate(TEAM_BATTING_SO = imputed.data[, 6],
         TEAM_BASERUN_SB = imputed.data[, 7],
         TEAM_PITCHING_SO = imputed.data[, 11],
         TEAM_FIELDING_DP = imputed.data[, 13]
         )

money.ball %<>%
  select(-INDEX)
```

<div class='page-header text-uppercase'>
  <h3>BUILD MODEL</h3>
</div>

<div class='alert alert-info'>
The first model we will build consists of every single predictor provided.
</div>

```{r}
l.all <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR +
              TEAM_BATTING_BB + TEAM_BATTING_SO + TEAM_BASERUN_SB + log(TEAM_PITCHING_H) +
              TEAM_PITCHING_HR + log(TEAM_PITCHING_BB) + log(TEAM_PITCHING_SO) + TEAM_FIELDING_E + 
              TEAM_FIELDING_DP, money.ball)
summary(l.all)
```

$$\widehat{WINS}=0.03\times BATTING_H + -0.02\times BATTING_{2B} + 0.09\times BATTING_{3B} + 0.11\times BATTING_{HR} + $$

$$ 0.06\times BATTING_{BB} + -0.03\times BATTING_{SO} + 0.04\times BASERUN_{SB} + 18.47\times log(PITCHING_{H}) + $$

$$ -0.03\times PITCHING_{HR} + -23.85\times log(PITCHING_{BB}) + 17.79\times log(PITCHING_{SO}) + -0.04\times FIELDING_{E} + $$

$$ -0.11\times FIELDING_{DP} - 69.94$$

<div class='well'>
This model meets all the required assumptions for regression [see appendix]. However, there is much left to be desired by this model. First, it is incredibly complex, featuring 13 predictors, including 1 that is not statistically significant. Many of the predictors are highly correlated. There are a few instances of simpsons perceived paradox in the coefficients of the predictors. 'Doubles' (TEAM_BATTING_2B) has a negative slope which seems to indicate that as more doubles are hit, the number of wins decreases and 'hits allowed' (TEAM_PITCHING_H) has a positive slope which seems to indicate that as more hits are allowed, the number of wins increases.

This model is a good baseline, but it can be improved.
</div>

<div class='alert alert-info'>
The second model will be built with backwards selection.
</div>

```{r}
l.back <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR +
              TEAM_BATTING_BB + TEAM_BATTING_SO + TEAM_BASERUN_SB + log(TEAM_PITCHING_H) +
              log(TEAM_PITCHING_BB) + log(TEAM_PITCHING_SO) + TEAM_FIELDING_E + 
              TEAM_FIELDING_DP, money.ball)
summary(l.back)
```

$$\widehat{WINS}=0.04\times BATTING_H + -0.02\times BATTING_{2B} + 0.09\times BATTING_{3B} + 0.07\times BATTING_{HR} + $$
$$ 0.06\times BATTING_{BB} + -0.03\times BATTING_{SO} + 0.04\times BASERUN_{SB} + 17.01\times log(PITCHING_H) +$$
$$ -25.16\times log(PITCHING_{BB}) + 17.61\times log(PITCHING_{SO}) + $$ 
$$ -0.04\times FIELDING_{E} + -0.11\times FIELDING_{DP} - 52.55$$
<div class='well'>
This model also meets all the assumptions for regression [see appendix]. This model is an improvement over the previous model, but only slightly. Backwards selection has removed 1 predictor but remains just as accurate as the first model. The $R^2$ of both models is about $0.32$. 

We will try to get a bit agressive with the final model.
</div>

<div class='alert alert-info'>
The third model will be built with newly created variables and provide me with some added leeway in making model decisions.
</div>

<div class='well'>
I began by combining all the positive offensive statistics. Each statistic is weighted by the number of bases a player advances. Singles, walks, and stolen base are weighted 1, doubles are weighted 2, triples are weighted 3 and homeruns are weighted 4. The new observation is called TEAM_BATTING_POS. The goals is to represent the number of bases of advancement each team produces each season. 
</div>

```{r}
money.ball.2 <- money.ball %>%
  mutate(TEAM_BATTING_POS = TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BASERUN_SB + TEAM_BATTING_2B * 1 + 
           TEAM_BATTING_3B * 2 + TEAM_BATTING_HR * 3) %>%
  dplyr::select(-TEAM_BATTING_H, -TEAM_BATTING_BB, -TEAM_BATTING_2B, -TEAM_BATTING_3B, -TEAM_BATTING_HR, -TEAM_BASERUN_SB)

l.combine <- lm(TARGET_WINS ~ TEAM_BATTING_SO + log(TEAM_PITCHING_BB) + log(TEAM_PITCHING_SO) + 
                  TEAM_FIELDING_E + TEAM_BATTING_POS + TEAM_FIELDING_DP, money.ball.2)
summary(l.combine) 
```

$$\widehat{WINS}=0.03\times BATTING_{POS} + -0.04\times BATTING_{SO} + -10.60\times log(PITCHING_{BB}) $$
$$+ 17.86\times log(PITCHING_{SO}) + -0.02\times FIELDING_{E} + -0.12\times FIELDING_{DP} - 3.21$$

<div class='well'>
This model meets all the assumptions for regression. [see appendix]

While this model does lose some predictive power with an $R^2$ of $0.30$ down from $0.32$, it may be considered a worthy tradeoff. First, it has the fewest predictors of any of the models at 6. In addition, none of the predictors fall victim to simpson's perceived paradox and thus make intuitive and logical sense. This may aid in understanding for general managers and non-statistically inclinded people. Each predictor is highly significant, indicating that we have boiled down the information to the most critical pieces of information.

There does still appears to be high collinearity in the model, however, this was present in all 3 of the created models.
</div>

<div class='page-header text-uppercase'>
  <h3>SELECT MODELS</h3>
</div>

<div class='well'>
Ultimately, I selected the third model. While it does have the least predictive power, that small loss is offset by the intuitive logic of the model and it's more simple make up. 

There are a number of issues with the dataset that should be considered. The data provided was collected over a large period of time yet it lacked the time information. Thus the assumption that the residuals are independent could not be checked. This is important as the data was collected over a long period of time and the game of baseball has changed immensly in that time. Just how helpful is data from the 1800s for predicting games in 2000? I would imagine very little. 

The model was used to predict the data of the test set. A sample of the results are below.
</div>

```{r, message=FALSE}
test.set <- read_csv('C:\\Users\\Brian\\Desktop\\GradClasses\\Summer18\\621\\621week1\\moneyball-evaluation-data.csv') 

dummy.vars <- dummyVars(~ ., data = test.set[, -1])
train.dummy <- predict(dummy.vars, test.set[, -1])

pre.process <- preProcess(train.dummy, method='bagImpute')
imputed.data <- predict(pre.process, train.dummy)

test.set %<>%
  mutate(TEAM_BATTING_SO = imputed.data[, 6],
         TEAM_BASERUN_SB = imputed.data[, 7],
         TEAM_PITCHING_SO = imputed.data[, 13],
         TEAM_FIELDING_DP = imputed.data[, 15]
         ) %>%
  mutate(TEAM_BATTING_POS = TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_BASERUN_SB + TEAM_BATTING_2B * 1 + 
           TEAM_BATTING_3B * 2 + TEAM_BATTING_HR * 3) %>%
  select(-TEAM_BATTING_H, -TEAM_BATTING_BB, -TEAM_BATTING_2B, -TEAM_BATTING_3B, -TEAM_BATTING_HR, -TEAM_BASERUN_SB, -INDEX, -TEAM_BASERUN_CS, -TEAM_BATTING_HBP)

predict(l.combine, newdata = test.set, interval='prediction') %>%
  head(10) %>%
  kable('html') %>%
  kable_styling(bootstrap_options = c('striped', 'hover'))
```

<div class='alert alert-success'>
In conclusion, a successfull regression was fit using 6 predictors with an $R^2$ of $0.3047$. This information was used to make predictions for a test set. The model is ready for deployment by the general managers however I wish to stress that without verifying the effect of collecting information over the course of a century, there are still concerns regarding the complete veracity of the information. Also, there remains high collinearity between the predictors that should be addressed.
</div>

<div class='page-header text-uppercase'>
  <h3>Appendix</h3>
</div>

<div class='well'>
Assumptions must be validated to ensure that the regression model is valid. A light review was the first two models is followed by a more indepth review of the final, selected model.
</div>

<div class='alert alert-info'>
Model #1
</div>

<div class='well'>
After log transforming 3 of the predictors, all the of the predictors are linearly related to the response variable. The data is roughly normal although there are upper outliers with a long tail. The data is homoscedactic.

Unfortunately the collinearity is very high in many of the predictors. This will need to be addressed.
</div>

```{r}
par(mfrow=c(2,2))
plot(l.all)
car::residualPlots(l.all)
faraway::vif(model.matrix(l.all)[, -1])
```

<div class='alert alert-info'>
Model #2
</div>

<div class='well'>
Due to the high level of similiarity to the previous model, there is no major difference between the analysis of this model and the previous model. This is due to the fact that the backwards selection was only able to elminate one variable.

An anova test demonstrates that there is no evidence of a statistically significant difference between the two models.
</div>

```{r}
anova(l.all, l.back)
```

<div class='alert alert-info'>
Model #3
</div>

<div class='well'>
All the of the predictors are linearly related to the response variable as seen in the scatterplots. The data is very close to normal with minimal outliers. The data is homoscedactic. The residual plots show homoscedasticity across all predictors and the final fitted values. 

Collinearity is not nearly as bad as the previous models but is still higher than we would like for 2 predictors. This will need to be addressed.
</div>

```{r}
pairs(TARGET_WINS ~ TEAM_BATTING_SO + log(TEAM_PITCHING_H) + log(TEAM_PITCHING_BB) + 
        log(TEAM_PITCHING_SO) + TEAM_FIELDING_E + TEAM_BATTING_POS + TEAM_FIELDING_DP, money.ball.2) 
par(mfrow=c(2,2))
plot(l.combine)
car::residualPlots(l.combine) 
faraway::vif(model.matrix(l.combine)[, -1]) 
```

<div class='well'>
Finally, the termplot shows that our transformation selections are all strong.
</div>

```{r}
par(mfrow=c(2,3))
termplot(l.combine, partial.resid=TRUE, smooth=panel.smooth, terms=1:6)
```